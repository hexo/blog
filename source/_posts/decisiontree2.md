---
title: 决策树学习的常见问题
date: 2016-10-24 10:43:03
tags:
categories: 机器学习
---

决策树学习的实际问题包括:确定决策树增长的深度,处理连续值的属性,选择一个适合的属性筛选度量标准,处理属性值不完整的训练数据,处理不同代价的属性,提高计算效率.下面将会分别介绍,事实上,为了解决多数问题,ID3算法已经被扩展成为C4.5

### 避免过度拟合
对于一个假设,当存在其他的假设对训练样例的拟合比它差,但事实上在实例的整体分布上却表现的更好时,我们说这个假设过度拟合训练样例

有几种途径可以被用来避免决策树学习中的过度拟合.他们可以被分成两类:
- 及早停止增长: 在ID3算法完美分类训练数据之前就停止树增长;
- 后修剪法: 允许树过度拟合数据,然后对这个树进行后修剪;

尽管第一种方法看起来很厉害的样子,但我们想精确的估计何时停止增长很困难,实际中广泛采用的是第二种思想
不过无论使用哪种方法,一个关键的问题是使用什么样的准则来确定最终正确树的规模,解决这个问题的方法包括:
- 使用与训练样例不同的一套分离的样例,来评估通过后修剪方法从树上修剪结点的效用
- 使用所有可用数据进行训练,但进行统计测试来估计扩展一个特定的节点是否有可能改善在训练集合外的实例上的性能
- 使用一个明确的衡量标准来衡量训练样例和决策树的复杂度,当这个编码最小的时候停止树增长,这个方法基于一种启发式规则,被称为最小描述长度

上面的第一种方法,常被称为训练和验证集,可用的数据被分成两个样例集合,一个训练集合用来形成学习到的假设,一个分离的验证集合用来评估这个假设在后续的数据上的精度,这样即使学习器可能会被训练中高的随机误差和巧合性规律所误导,但是验证集合不太可能会出先同样的随机波动,所以以此来对过度拟合训练集中的虚假特征提供防护检验
那么如何使用验证集合来防止过度拟合??

#### 方法1: 错误率降低修剪
考虑将树上每一个结点作为修剪的候选对象,修剪的步骤如下
- 删除以此结点为根的子树
- 使它成为叶子结点
- 把和该结点相关联的训练样例的最常见分类赋给它
- 最重要的是,修剪后的树对于验证集合的性能不比原来的树差时才可以删除

#### 方法2:规则后修剪
实践中,一种用来发现高精度假设的非常成功的方法为"规则后修剪",这中修剪方法的一种变体应用在C4.5中,具体包括下面的步骤
- 从训练集合推导出决策树,增长决策树直到尽可能好地拟合训练数据,允许过度拟合发生
- 将决策树转化为等价的规则集合,方法是为从根结点到叶子结点的每条路径创建一条规则
- 删除任何能导致估计精度提高的前件来修剪每一条规则
- 按照修剪过的规则的估计精度对他们进行排序,并按照这样的顺序应用这些规则来分类后来的实例

下面是关于前件后件的解释
![规则后修剪](http://oelvsay9f.bkt.clouddn.com/%E8%A7%84%E5%88%99%E5%90%8E%E4%BF%AE%E5%89%AA.png)

### 合并连续值属性
之前说到的ID3算法是限制取离散值的属性,不过我们可以把连续值的属性分割为离散的区间集合,例如连续值属性A,算法可以动态创建一个阈值c,如果A<c,那么A为真,否则A为假
那么问题就变成了如何去确定c?
对于某一个属性来说,我们无疑会选择信息增益最大的阈值,具体做法如下:
- 首先按照连续属性A排序样例,然后确定目标分类不同的相邻实例
- 于是我们产生了一组候选阈值,他们的值是A值之间的中间值
- 计算每个候选阈值关联的信息增益,选择信息增益最大的候选阈值

### 属性选择的度量标准
上面说到信息增益的度量标准存在一个内在偏置,即偏袒具有较多值的属性,但是存在一个问题,太多的可能值必然会把训练样例分割成非常小的空间
为了避免这个不足,我们可以采用其他度量标准比如 增益比率(gain ratio),增益比率通过加入一个被称作分裂信息的项来惩罚属性,分裂信息用来衡量属性分裂数据的广度和均匀性
![分裂信息公式](http://oelvsay9f.bkt.clouddn.com/%E5%88%86%E8%A3%82%E4%BF%A1%E6%81%AF%E5%85%AC%E5%BC%8F.png)
增益比率是用前面的信息度量和这里的分裂信息共同定义的
![增益比率](http://oelvsay9f.bkt.clouddn.com/%E5%A2%9E%E7%9B%8A%E6%AF%94%E7%8E%87.png)

### 缺少属性值的训练样例
一种方法是取当前训练样例中该属性最常见的值,另一种方法是给每一个可能的值一个概率,通过概率的方法来计算信息增益

### 处理不同代价的属性
有的属性代价很高,比如医疗检查,涉及到所需的费用和患者的不适.对于这样的任务,我们优先考虑使用低代价属性的决策树
另外我们可以用信息增益除以属性代价,这样就能够让低代价的属性优先被选择
