---
title: 爬虫代码分析
date: 2016-11-11 21:22:10
tags:
categories: python
---
针对上面的代码结构，进行以下几个模块的分析

### python库

- re 是关于正则匹配的库，可以完成字段的正则匹配和遍历满足条件的字符串
- urlparse 是进行url拼接，编码，解码相关的库
- urllib2 是我们进行数据抓取的核心模块
- robotparser 是解析域名的robot.txt文件时使用的，robot.txt文件定义了网站对于爬虫访问的权限
- builtwith 是一个检查网站构建技术类型的模块，比如某个网站使用了python的flask框架
- whois 检查网站的所有者，即该域名归属于谁

### link_crawler函数分析

首先是传入参数，

- seed_url 定义了传入的域名
- link_regex 为需要在该页面中进行正则匹配的子域名
- delay 设置了两次下载之间存在的时延，主要是为了防止爬取速度过快被服务器封禁或者过载的风险
- max_depth 设置了最大的访问深度，防止进入无穷链接的陷阱，造成死循环，目前为-1代表该项不启用
- max_urls 设置了最大可访问的url数目，同样也是为了防止数目太多，服务器过载现象的发生
- headers 设置请求头
- user_agent 设置用户代理
- proxy 设置代理
- num_retries: 如果访问失败是否重新尝试，尝试几次

### 顺序分析

- 把seed_url即我们初始设置的url放在待访问的双端队列中
- 设置当前看过的url dict为空
- 设置当前访问的url数目为空
- 获取 robot文件信息，时延信息，请求头信息，用户代理信息
- 当待访问的url队列信息不为空时，依次提取队列中每一个url进行访问，检测相关访问权限之后，下载页面内容，并从内页面信息中检测出匹配到的新的url链接，放入到队列中，当然这其中也进行了是否超过最大深度的判断，用户访问权限的判断，用户代理的设置等等

### 子函数分析

- get_links函数 这个函数把与目标要求满足正则匹配的链接提取出来
- get_robots 这个函数提取robots.txt文件并进行分析，检测当前用户是否有权限进行对目标网站的数据抓取
- download 这个是用于下载的函数 首先根据url，请求头，等信息向目标网站发起请求，然后对请求结果进行分析，根据返回的状态码对结果进行分类，当状态码500~600之间时会发起重新请求的指令，防止是由于服务器临时原因造成的数据抓取失败

### 总结

这样，通过这样几个基础的python库，我们已经能够实现抓取与某个链接相关联的全部内容，但是这样的数据是在是太多了，其中包含很多我们不需要的内容，接下来我们需要通过一些分析手段取抓取的数据进行分析，提取有价值的部分，这部分会在以后的文章中给出
