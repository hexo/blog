---
title: 浅谈决策树学习算法
date: 2016-10-24 09:25:40
tags:
categories: 机器学习
---

### 什么是决策树
决策树是应用最广泛的归纳推理算法之一,是一种逼近离散值函数的方法,对噪声数据有良好的健壮性并且能够学习析取表达式,主要包括ID3,ASSISTANT和C4.5这些广为应用的算法

### 决策树的表示方法
下图所示是一棵决策树,这棵决策树根据天气情况分类"星期六上午是否适合打网球",树上每个结点指定了对实例的每个属性的测试,叶子结点为实例所属的分类
![playTennis的决策树](http://oelvsay9f.bkt.clouddn.com/%E5%86%B3%E7%AD%96%E6%A0%91.gif)

### 决策树的适用问题
- 实例是由"属性-值"对来表示的,在最简单的决策树学习中,每个属性取少数的几个离散的值,然而后面扩展的算法也允许处理值为实数的属性
- 目标函数具有离散的输出值,我们从上图可见决策树输出布尔型的分类,yes or no,决策树方法很容易扩展学习到有两个以上输出值的函数
- 可能需要析取的描述,如上面指出的,决策树很自然代表了析取的表达式
- 训练数据可以包含错误,决策树学习对错误有很好的健壮性,因为决策树不单单靠一个属性就决定叶子结点的值
- 训练数据可以包含缺少属性值的实例,决策树可以在含有未知属性的训练集当中使用

### 决策树的学习算法ID3
ID3算法是决策树算法的核心算法,大多数已开发的决策树学习算法都是ID3算法的变体
ID3算法通过自顶向下构造决策树来进行学习.那么第一个问题来了.如何选取某个属性作为根节点?
为了解决这个问题,我们决定使用统计测试来确定每个属性单独分类训练样例的能力,然后选择能力最强的作为决策树的根节点,那么问题又来了...如何确定每个属性单独分类训练样例的能力?
我们先放下这个疑问,请读者思考一下.
现在假如我们已经找到了最强分类能力的属性A,我们把A放在根结点,然后针对A的每个可能的取值产生一个分支,把训练样例排列到对应的分支
现在我们在每个分支下,用该分支的训练样例再寻找分类能力最强的结点,以此类推,直到发现某个分支下所有训练样例的目标函数都相等,这也就达到了分类的目的

### 熵的概念
在决定属性的分类能力之前,先要了解一下熵的概念,这是信息论中广泛使用的一个度量标准,它刻画了任意样例集的纯度
![熵的定义](http://oelvsay9f.bkt.clouddn.com/%E7%86%B5.png)

所以我们发现了几个特点
- 熵的最大值: 即布尔型最大值为1,C个可能值的话就是log C
- 每个取值的概率相同时熵最大

### 哪个属性是最佳属性
衡量一个属性价值的定量标准是什么呢,这里有一个定义叫做"信息增益"(information gain),被用来衡量给定的属性区分训练样例的能力,我们每次选取信息增益最大的属性作为决策树的结点属性
我们已经有了熵作为衡量训练样例集合纯度的标准,一个属性的信息增益简单的说,就是由于使用这个属性分割样例而导致的期望熵降低程度,信息增益越大,分类能力越强
更精确的说,一个属性A相对与样例集合S的信息增益Gain(S,A)被定义为:
![信息增益的定义](http://oelvsay9f.bkt.clouddn.com/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A.png)

### 决策树的归纳偏置
ID3算法用什么策略从观测到的训练数据泛化以分类未见实例呢? 这个前提,也被称为归纳偏置,近似的ID3算法的归纳偏置为: 较短的树比较长的树优先
概括来讲,ID3算法的搜索策略是:
- 优先选择较短的树而不是较长的
- 选择那些信息增益高的属性离根节点较近的树

### 为什么短的假设优先
ID3算法中优选较短的决策树的归纳偏置,它的思想来源叫做'奥卡姆剃刀'
奥卡姆剃刀: 优先选择拟合数据的最简单的假设
还有一件事,读到这里可能读者已经发现了,我们在构建决策树是采用了贪心算法的思想,即每一次选择结点时我们总会选择当前所有属性中信息增益最大的属性作为结点,这其实很有可能造成局部最优
关于决策树学习的常见问题,会在后面的笔记中记录
